import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as opt


def sigmoid(z):
    # sigmoid = 1 / (1 + exp(-z))
    return 1 / (1 + np.exp(-z))


def cost_function(thetas, x, y):
    num_examples = np.shape(x)[0]

    hypothesis = sigmoid(np.dot(x, thetas))

    y_transpose = y.transpose()
    # j1 = -y * log(h(x)), (for y = 1)
    y_equals_1 = np.dot(-y_transpose, np.log(hypothesis))
    # j0 = -(1 - y) * log(1 - h(x)), (for y = 0)
    y_equals_0 = np.dot((1 - y_transpose), np.log(1 - hypothesis))
    # j = (j1 + j0) / m
    j = (y_equals_1 - y_equals_0) / num_examples

    return j


def cost_function_regularized(thetas, x, y, lambda_value):
    num_examples = np.shape(x)[0]

    j = cost_function(thetas, x, y)
    # j = j + lambda * thetas(j)^2 / 2 * m
    j = j + lambda_value * np.dot(thetas[1:].transpose(), thetas[1:]) / (2 * num_examples)

    return j


def gradient(thetas, x, y):
    num_examples = np.shape(x)[0]

    z = np.dot(x, thetas)
    hypothesis = sigmoid(z)
    # gradient = dj/d(theta(j)) = (h(x) - y) * x(j) / m
    gradient = np.dot(x.transpose(), (hypothesis - y)) / num_examples

    return gradient


def gradient_regularized(thetas, x, y, lambda_value):
    num_examples = np.shape(x)[0]

    grad = gradient(thetas, x, y)
    # gradient = dj/d(theta(j)) = (h(x) - y) * x(j) / m, (for j = 0)
    # gradient = dj/d(theta(j)) + lambda * thetas(j) / m, (for j >= 1)
    grad = grad + lambda_value * np.append(0, thetas[1:]) / num_examples

    return grad


def predict(thetas, x):
    hypothesis = sigmoid(np.dot(x, thetas))
    predict = hypothesis >= 0.5

    return predict


# feature mapping function to polynomial features
# map_feature(x1, x2) maps the two input features
# to quadratic features used in the regularization exercise.
# returns a new feature array with more features, comprising of
# x1, x2, x1.^2, x2.^2, x1*x2, x1*x2.^2, etc..
# Inputs x1, x2 must be the same size
def map_feature(x1, x2):
    degree = 6
    # an alternative of sum would be: ((degree+1)*(degree+2))/2
    out = np.ones((x1.shape[0], sum(range(degree + 2))))
    current_column = 1
    for i in range(1, degree + 1):
        for j in range(i + 1):
            out[:, current_column] = np.power(x1, i - j) * np.power(x2, j)
            current_column += 1

    return out


def gradient_descent(x, y, thetas, alpha, max_iterations):
    num_examples, num_features = np.shape(x)

    # j_history is for plot convergence graph
    j_history = np.zeros([max_iterations, 1])

    for i in range(max_iterations):
        # create a copy of thetas
        # for simultaneously update
        thetas_previous = thetas

        for j in range(num_features):
            # calculate dj/d(theta(j)) = (h(x) - y) * x(j) / m, (h(x) = sigmoid(x))
            derivative = ((sigmoid(np.dot(x, thetas_previous)) - y).transpose()) * x[:, j] / num_examples
            # simultaneously update theta(j)
            thetas[j] = thetas_previous[j] - alpha * derivative.sum()

        # save the values of j of each iteration
        j_history[i] = cost_function(thetas, x, y)

    return thetas, j_history


def plot_convergence_graph(j):
    # plot convergence graph
    plt.plot(list(range(1, np.size(j) + 1)), j[:, 0], color="r")

    # put labels
    plt.xlabel('Number of iterations')
    plt.ylabel('Cost J')

    # show plot
    plt.show()


if __name__ == "__main__":
    # load data set
    # data set without the needs of regularization
    x = np.loadtxt('ex2data1.txt', delimiter=',', usecols=(0, 1))
    y = np.loadtxt('ex2data1.txt', delimiter=',', usecols=2)

    # extract the number of examples and features
    num_examples, num_features = np.shape(x)

    # concatenate an all ones vector as the first column
    x = np.hstack((np.ones((len(y), 1)), x.reshape(num_examples, num_features)))

    # initialize thetas
    thetas = np.zeros(num_features + 1)

    # calculate cost and gradient
    print('Cost at thetas:\n', cost_function(thetas, x, y))
    print('Gradient at thetas:\n', gradient(thetas, x, y))

    # an alternative approach to fminunc in Octave/MATLAB by scipy.optimize
    # https://docs.scipy.org/doc/scipy/reference/optimize.html
    # https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html
    # https://docs.scipy.org/doc/scipy-0.10.0/reference/tutorial/optimize.html
    optimized = opt.minimize(fun=cost_function, x0=thetas, args=(x, y), method='TNC', jac=gradient)
    print('\nOver all results found by scipy.optimize is:\n', optimized)
    print('\nThe optimized thetas found by scipy.optimize is:\n', optimized.x)
    print('\nThe cost at thetas found by scipy.optimize is:\n', optimized.fun)

    # calculate train accuracy by the optimized thetas generated by scipy.optimize
    p = predict(optimized.x, x)
    print('\nThe train accuracy by scipy.optimize is:\n', np.mean(p == y) * 100)

    # learning rate and the maximum iteration
    # should be tuned based on varying data set
    alpha = 0.002
    max_iterations = 8000000

    # calculate gradient descent and save the value of j
    # for each iteration for plotting convergence graph
    gradient_descent = gradient_descent(x, y, thetas, alpha, max_iterations)

    print('\nThe thetas generated by alpha={} and max_iteration={} is:\n'.format(alpha, max_iterations), gradient_descent[0])

    # plot convergence graph
    plot_convergence_graph(gradient_descent[1])

    '''
    # load data set
    # data set needs regularization
    x = np.loadtxt('ex2data2.txt', delimiter=',', usecols=(0, 1))
    y = np.loadtxt('ex2data2.txt', delimiter=',', usecols=2)

    # return a new feature array with more features
    x = map_feature(x[:, 0], x[:, 1])

    # extract the number of examples
    num_features = np.shape(x)[1]

    # initialize thetas
    # an alternative would be thetas = np.ones(num_features)
    thetas = np.zeros(num_features)
    thetas = np.ones(num_features)

    # set a regularized parameter
    # an alternative value would be 10
    lambda_value = 10

    print('Cost at thetas (with lambda = {}):\n'.format(lambda_value), cost_function_regularized(thetas, x, y, lambda_value))
    print('Gradient at thetas (with lambda = {}):\n'.format(lambda_value), gradient_regularized(thetas, x, y, lambda_value))
    '''

    # the regularized gradient can be also verified by plotting a convergence graph
    # a similar gradient_descent function can be applied for this propose

    # for plot the decision boundary refer to:
    # https://github.com/arturomp/coursera-machine-learning-in-python/blob/master/mlclass-ex2-004/mlclass-ex2/plotDecisionBoundary.py
